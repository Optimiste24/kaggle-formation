{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb570c20-e67f-48a2-a7e3-04c3207bfd64",
   "metadata": {},
   "source": [
    "#  Forêts aléatoires !\n",
    "\n",
    "À l’aide d’un algorithme d’apprentissage automatique plus sophistiqué."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d452f-f3ea-424a-afd4-e6437f2a61a0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Les **arbres de décision** vous laissent face à une décision difficile. Un arbre profond avec de nombreuses feuilles surajuste car chaque prédiction provient de données historiques de seulement quelques maisons à sa feuille. En revanche, un arbre superficiel avec peu de feuilles performe mal car il ne parvient pas à capturer suffisamment de distinctions dans les données brutes.\n",
    "\n",
    "Même les techniques de modélisation les plus sophistiquées d'aujourd'hui font face à cette tension entre **sous-ajustement et surajustement**. Cependant, de nombreux modèles ont des idées astucieuses qui peuvent conduire à de meilleures performances. Nous allons examiner ***la forêt aléatoire (random forest)*** comme exemple.\n",
    "\n",
    "La forêt aléatoire utilise de nombreux arbres et fait une prédiction en faisant la moyenne des prédictions de chaque arbre composant. Elle offre généralement une précision prédictive bien meilleure qu'un seul arbre de décision et fonctionne bien avec les paramètres par défaut. En poursuivant la modélisation, vous pouvez apprendre d'autres modèles avec des performances encore meilleures, mais beaucoup d'entre eux sont sensibles au choix des bons paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cdc24-bae7-42f3-93f9-a919a66aa7bf",
   "metadata": {},
   "source": [
    "**Exemple**\n",
    "\n",
    "Vous avez déjà vu le code pour charger les données plusieurs fois. À la fin du chargement des données, nous avons les variables suivantes :\n",
    "\n",
    "- train_X\n",
    "- val_X\n",
    "- train_y\n",
    "- val_y\n",
    "\n",
    "Nous construisons un modèle de forêt aléatoire de manière similaire à la manière dont nous avons construit un arbre de décision dans scikit-learn - cette fois en utilisant la classe ***RandomForestRegressor*** au lieu de **DecisionTreeRegressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe9a813-c1ec-47b8-a7bc-d01153f461bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207190.6873773146\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Charger les données\n",
    "melbourne_file_path = 'melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# Supprimer les lignes avec des valeurs manquantes\n",
    "melbourne_data1 = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Définir la cible et les features\n",
    "y = melbourne_data1.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "X = melbourne_data1[melbourne_features]\n",
    "\n",
    "\n",
    "# Définir le modèle de forêt aléatoire avec un état aléatoire pour la reproductibilité\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Division des données (données entrainement et validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "forest_model.fit(train_X, train_y)\n",
    "\n",
    "# Faire des prédictions sur les données de validation\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "\n",
    "# Calculer et afficher l'erreur absolue moyenne\n",
    "print(mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72cc7f-60e0-4f43-b03d-5e624e76c3d1",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Il y a probablement encore des améliorations possibles, mais c'est une grande amélioration par rapport à l'erreur de l'arbre de décision le plus performant, qui était de 273 681,46. Il existe des paramètres qui vous permettent de modifier les performances de la forêt aléatoire de la même manière que nous avons ajusté la profondeur maximale de l'arbre de décision unique. Mais **l'un des meilleurs avantages des modèles de forêt aléatoire est qu'ils fonctionnent généralement de manière raisonnable même sans cet ajustement.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a16ec-7951-4b13-9299-e3149c119936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
