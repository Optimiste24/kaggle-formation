{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d155b-3ff0-416e-a627-79beec81e543",
   "metadata": {},
   "source": [
    "# XGBoost : La technique de mod√©lisation la plus pr√©cise pour les donn√©es structur√©es\n",
    "\n",
    "Dans ce tutoriel, vous apprendrez √† construire et optimiser des mod√®les avec le **gradient boosting**. Cette m√©thode domine de nombreuses comp√©titions Kaggle et offre des r√©sultats de pointe sur une vari√©t√© de jeux de donn√©es.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c3558-355b-486e-be6c-a99ea38b96e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Dans une grande partie de ce cours, vous avez fait des pr√©dictions avec la m√©thode des for√™ts al√©atoires, qui offrent de meilleures performances qu'un seul arbre de d√©cision simplement en moyennant les pr√©dictions de plusieurs arbres.\n",
    "\n",
    "Nous appelons la m√©thode des for√™ts al√©atoires une m√©thode d'ensemble (ensemble method). Par d√©finition, les m√©thodes d'ensemble combinent les pr√©dictions de plusieurs mod√®les (par exemple, plusieurs arbres, dans le cas des for√™ts al√©atoires).\n",
    "\n",
    "Ensuite, nous allons apprendre une autre m√©thode d'ensemble appel√©e **gradient boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698c946-ba98-4fa1-a488-d3fb27816105",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gradient Boosting\n",
    "Le gradient boosting est une m√©thode qui fonctionne en ajoutant des mod√®les dans un ensemble de mani√®re it√©rative.\n",
    "\n",
    "### 1. Initialisation : \n",
    "L'ensemble commence avec un seul mod√®le, dont les pr√©dictions peuvent √™tre tr√®s basiques. (M√™me si ses pr√©dictions sont tr√®s inexactes, les ajouts suivants corrigeront ces erreurs.)\n",
    "\n",
    "### 2. Cycle it√©ratif :\n",
    "- Utilisez l'ensemble actuel pour g√©n√©rer des pr√©dictions pour chaque observation dans les donn√©es.\n",
    "- Calculez une fonction de perte (comme l'erreur quadratique moyenne) avec ces pr√©dictions.\n",
    "- Ajustez un nouveau mod√®le pour r√©duire la perte en d√©terminant ses param√®tres via la descente de gradient.\n",
    "- Ajoutez ce nouveau mod√®le √† l'ensemble et recommencez le cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1781a-e455-429b-9d7a-ecb704270978",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exemple\n",
    "Nous commen√ßons par charger les donn√©es d'entra√Ænement et de validation dans X_train, X_valid, y_train, et y_valid.\n",
    "\n",
    "Dans cet exemple, vous utiliserez la biblioth√®que XGBoost, qui signifie **\"extreme gradient boosting\"**. C'est une impl√©mentation de gradient boosting avec des fonctionnalit√©s suppl√©mentaires ax√©es sur la performance et la rapidit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333da02a-a8ce-4cbe-b6c8-24e63dfca28c",
   "metadata": {},
   "source": [
    "**Code de base pour XGBoost :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b858c42-4955-442a-bbd2-a47c9e0976f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.4/124.9 MB 16.8 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.6/124.9 MB 19.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 12.3/124.9 MB 22.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 14.9/124.9 MB 19.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 20.7/124.9 MB 21.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 27.3/124.9 MB 23.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 25.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.0/124.9 MB 28.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 50.9/124.9 MB 28.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 58.5/124.9 MB 29.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 65.0/124.9 MB 29.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 71.6/124.9 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.2/124.9 MB 30.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.8/124.9 MB 30.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 93.8/124.9 MB 31.0 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 100.7/124.9 MB 31.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 108.0/124.9 MB 31.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 115.1/124.9 MB 31.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.2/124.9 MB 31.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 32.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 32.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 28.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a278fa2-888b-422c-9993-a094e7095a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur absolue moyenne : 164091.43196566642\n"
     ]
    }
   ],
   "source": [
    "# Importation des biblioth√®ques n√©cessaires\n",
    "from sklearn.compose import ColumnTransformer  # Pour transformer les colonnes sp√©cifiques\n",
    "from sklearn.impute import SimpleImputer  # Pour g√©rer les valeurs manquantes\n",
    "from sklearn.preprocessing import OneHotEncoder  # Pour encoder les colonnes cat√©goriques\n",
    "from sklearn.pipeline import Pipeline  # Pour cr√©er un pipeline d'√©tapes\n",
    "from sklearn.model_selection import train_test_split  # Pour diviser les donn√©es en ensembles d'entra√Ænement et de validation\n",
    "from sklearn.metrics import mean_absolute_error  # Pour √©valuer le mod√®le avec l'erreur absolue moyenne\n",
    "from xgboost import XGBRegressor  # Mod√®le de r√©gression XGBoost\n",
    "import pandas as pd  # Pour manipuler les donn√©es\n",
    "\n",
    "# √âtape 1 : Chargement des donn√©es\n",
    "# Le chemin du fichier CSV contenant les donn√©es de Melbourne\n",
    "melbourne_file_path = 'melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# √âtape 2 : Pr√©paration des donn√©es\n",
    "# Supprimer les lignes o√π la cible ('Price') est manquante\n",
    "melbourne_data.dropna(axis=0, subset=['Price'], inplace=True)\n",
    "\n",
    "# S√©parer la cible (y) des pr√©dicteurs (X)\n",
    "y = melbourne_data['Price']  # Variable cible\n",
    "X = melbourne_data.drop(['Price'], axis=1)  # Donn√©es pr√©dicteurs\n",
    "\n",
    "# Identifier les colonnes num√©riques et cat√©goriques dans X\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# √âtape 3 : Transformation des donn√©es\n",
    "# Cr√©ation d'un transformateur pour g√©rer les colonnes num√©riques et cat√©goriques\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Pour les colonnes num√©riques : remplacer les valeurs manquantes par la moyenne\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_cols),\n",
    "        # Pour les colonnes cat√©goriques : appliquer un encodage OneHot en ignorant les cat√©gories inconnues\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# √âtape 4 : Cr√©ation du pipeline\n",
    "# Le mod√®le utilis√© dans ce pipeline est XGBRegressor\n",
    "model = XGBRegressor(random_state=0)  # R√©gression XGBoost avec un param√®tre d'al√©atoire fix√©\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # √âtape de pr√©traitement\n",
    "    ('model', model)  # √âtape de mod√©lisation\n",
    "])\n",
    "\n",
    "# √âtape 5 : Division des donn√©es\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# √âtape 6 : Entra√Ænement du pipeline\n",
    "# Le pipeline applique le pr√©traitement et entra√Æne le mod√®le en une seule commande\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# √âtape 7 : Pr√©diction et √©valuation\n",
    "# Pr√©dire les valeurs cibles pour l'ensemble de validation\n",
    "predictions = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Calculer l'erreur absolue moyenne (Mean Absolute Error)\n",
    "print(\"Erreur absolue moyenne : \" + str(mean_absolute_error(predictions, y_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1627a1-1d31-4293-85ae-b971b002b884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0b04e-bd2b-461b-96bd-736754d828df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Code de base pour XGBoost :**\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Mod√®le initial\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "# √âvaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Erreur absolue moyenne : \" + str(mean_absolute_error(predictions, y_valid)))\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Param√®tres √† ajuster dans XGBoost\n",
    "\n",
    "#### **n_estimators**\n",
    "- Sp√©cifie combien de fois le cycle de mod√©lisation est ex√©cut√© (nombre de mod√®les ajout√©s √† l'ensemble).\n",
    "- Valeurs typiques : 100-1000 (selon la taille des donn√©es et la valeur de `learning_rate`).\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **early_stopping_rounds**\n",
    "- Interrompt automatiquement l'entra√Ænement si le score de validation ne s'am√©liore plus apr√®s un certain nombre d'it√©rations.\n",
    "- Utilise un ensemble de validation pour calculer les scores (`eval_set`).\n",
    "\n",
    "Exemple avec arr√™t anticip√© :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **learning_rate**\n",
    "- Contr√¥le l'impact de chaque mod√®le dans l'ensemble.\n",
    "- Une petite valeur de `learning_rate` combin√©e √† un grand `n_estimators` donne des mod√®les plus pr√©cis mais prend plus de temps √† entra√Æner.\n",
    "- Valeur par d√©faut : `learning_rate=0.1`.\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **n_jobs**\n",
    "- Permet d'utiliser le parall√©lisme pour acc√©l√©rer l'entra√Ænement.\n",
    "- Configurez `n_jobs` au nombre de c≈ìurs de votre machine.\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "XGBoost est une biblioth√®que puissante pour les donn√©es tabulaires (celles stock√©es dans des DataFrames Pandas). Avec un ajustement minutieux des param√®tres, vous pouvez entra√Æner des mod√®les tr√®s pr√©cis.\n",
    "\n",
    "---\n",
    "\n",
    "### √Ä vous de jouer¬†!\n",
    "\n",
    "Entra√Ænez votre propre mod√®le avec XGBoost dans le prochain exercice ! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecf59c-5ee9-4956-b80b-ee2a1bc3cecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
