{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d155b-3ff0-416e-a627-79beec81e543",
   "metadata": {},
   "source": [
    "# XGBoost : La technique de modélisation la plus précise pour les données structurées\n",
    "\n",
    "Dans ce tutoriel, vous apprendrez à construire et optimiser des modèles avec le **gradient boosting**. Cette méthode domine de nombreuses compétitions Kaggle et offre des résultats de pointe sur une variété de jeux de données.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c3558-355b-486e-be6c-a99ea38b96e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Dans une grande partie de ce cours, vous avez fait des prédictions avec la méthode des forêts aléatoires, qui offrent de meilleures performances qu'un seul arbre de décision simplement en moyennant les prédictions de plusieurs arbres.\n",
    "\n",
    "Nous appelons la méthode des forêts aléatoires une méthode d'ensemble (ensemble method). Par définition, les méthodes d'ensemble combinent les prédictions de plusieurs modèles (par exemple, plusieurs arbres, dans le cas des forêts aléatoires).\n",
    "\n",
    "Ensuite, nous allons apprendre une autre méthode d'ensemble appelée **gradient boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698c946-ba98-4fa1-a488-d3fb27816105",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gradient Boosting\n",
    "Le gradient boosting est une méthode qui fonctionne en ajoutant des modèles dans un ensemble de manière itérative.\n",
    "\n",
    "### 1. Initialisation : \n",
    "L'ensemble commence avec un seul modèle, dont les prédictions peuvent être très basiques. (Même si ses prédictions sont très inexactes, les ajouts suivants corrigeront ces erreurs.)\n",
    "\n",
    "### 2. Cycle itératif :\n",
    "- Utilisez l'ensemble actuel pour générer des prédictions pour chaque observation dans les données.\n",
    "- Calculez une fonction de perte (comme l'erreur quadratique moyenne) avec ces prédictions.\n",
    "- Ajustez un nouveau modèle pour réduire la perte en déterminant ses paramètres via la descente de gradient.\n",
    "- Ajoutez ce nouveau modèle à l'ensemble et recommencez le cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1781a-e455-429b-9d7a-ecb704270978",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exemple\n",
    "Nous commençons par charger les données d'entraînement et de validation dans X_train, X_valid, y_train, et y_valid.\n",
    "\n",
    "Dans cet exemple, vous utiliserez la bibliothèque XGBoost, qui signifie **\"extreme gradient boosting\"**. C'est une implémentation de gradient boosting avec des fonctionnalités supplémentaires axées sur la performance et la rapidité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333da02a-a8ce-4cbe-b6c8-24e63dfca28c",
   "metadata": {},
   "source": [
    "**Code de base pour XGBoost :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b858c42-4955-442a-bbd2-a47c9e0976f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.4/124.9 MB 16.8 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.6/124.9 MB 19.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 12.3/124.9 MB 22.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 14.9/124.9 MB 19.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 20.7/124.9 MB 21.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 27.3/124.9 MB 23.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 35.4/124.9 MB 25.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 44.0/124.9 MB 28.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 50.9/124.9 MB 28.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 58.5/124.9 MB 29.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 65.0/124.9 MB 29.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 71.6/124.9 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.2/124.9 MB 30.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.8/124.9 MB 30.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 93.8/124.9 MB 31.0 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 100.7/124.9 MB 31.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 108.0/124.9 MB 31.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 115.1/124.9 MB 31.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.2/124.9 MB 31.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 32.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 32.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 28.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a278fa2-888b-422c-9993-a094e7095a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur absolue moyenne : 164091.43196566642\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from sklearn.compose import ColumnTransformer  # Pour transformer les colonnes spécifiques\n",
    "from sklearn.impute import SimpleImputer  # Pour gérer les valeurs manquantes\n",
    "from sklearn.preprocessing import OneHotEncoder  # Pour encoder les colonnes catégoriques\n",
    "from sklearn.pipeline import Pipeline  # Pour créer un pipeline d'étapes\n",
    "from sklearn.model_selection import train_test_split  # Pour diviser les données en ensembles d'entraînement et de validation\n",
    "from sklearn.metrics import mean_absolute_error  # Pour évaluer le modèle avec l'erreur absolue moyenne\n",
    "from xgboost import XGBRegressor  # Modèle de régression XGBoost\n",
    "import pandas as pd  # Pour manipuler les données\n",
    "\n",
    "# Étape 1 : Chargement des données\n",
    "# Le chemin du fichier CSV contenant les données de Melbourne\n",
    "melbourne_file_path = 'melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "# Étape 2 : Préparation des données\n",
    "# Supprimer les lignes où la cible ('Price') est manquante\n",
    "melbourne_data.dropna(axis=0, subset=['Price'], inplace=True)\n",
    "\n",
    "# Séparer la cible (y) des prédicteurs (X)\n",
    "y = melbourne_data['Price']  # Variable cible\n",
    "X = melbourne_data.drop(['Price'], axis=1)  # Données prédicteurs\n",
    "\n",
    "# Identifier les colonnes numériques et catégoriques dans X\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Étape 3 : Transformation des données\n",
    "# Création d'un transformateur pour gérer les colonnes numériques et catégoriques\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Pour les colonnes numériques : remplacer les valeurs manquantes par la moyenne\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_cols),\n",
    "        # Pour les colonnes catégoriques : appliquer un encodage OneHot en ignorant les catégories inconnues\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Étape 4 : Création du pipeline\n",
    "# Le modèle utilisé dans ce pipeline est XGBRegressor\n",
    "model = XGBRegressor(random_state=0)  # Régression XGBoost avec un paramètre d'aléatoire fixé\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Étape de prétraitement\n",
    "    ('model', model)  # Étape de modélisation\n",
    "])\n",
    "\n",
    "# Étape 5 : Division des données\n",
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Étape 6 : Entraînement du pipeline\n",
    "# Le pipeline applique le prétraitement et entraîne le modèle en une seule commande\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Étape 7 : Prédiction et évaluation\n",
    "# Prédire les valeurs cibles pour l'ensemble de validation\n",
    "predictions = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Calculer l'erreur absolue moyenne (Mean Absolute Error)\n",
    "print(\"Erreur absolue moyenne : \" + str(mean_absolute_error(predictions, y_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1627a1-1d31-4293-85ae-b971b002b884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0b04e-bd2b-461b-96bd-736754d828df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Code de base pour XGBoost :**\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Modèle initial\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Erreur absolue moyenne : \" + str(mean_absolute_error(predictions, y_valid)))\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Paramètres à ajuster dans XGBoost\n",
    "\n",
    "#### **n_estimators**\n",
    "- Spécifie combien de fois le cycle de modélisation est exécuté (nombre de modèles ajoutés à l'ensemble).\n",
    "- Valeurs typiques : 100-1000 (selon la taille des données et la valeur de `learning_rate`).\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **early_stopping_rounds**\n",
    "- Interrompt automatiquement l'entraînement si le score de validation ne s'améliore plus après un certain nombre d'itérations.\n",
    "- Utilise un ensemble de validation pour calculer les scores (`eval_set`).\n",
    "\n",
    "Exemple avec arrêt anticipé :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **learning_rate**\n",
    "- Contrôle l'impact de chaque modèle dans l'ensemble.\n",
    "- Une petite valeur de `learning_rate` combinée à un grand `n_estimators` donne des modèles plus précis mais prend plus de temps à entraîner.\n",
    "- Valeur par défaut : `learning_rate=0.1`.\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **n_jobs**\n",
    "- Permet d'utiliser le parallélisme pour accélérer l'entraînement.\n",
    "- Configurez `n_jobs` au nombre de cœurs de votre machine.\n",
    "\n",
    "Exemple :\n",
    "```python\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "XGBoost est une bibliothèque puissante pour les données tabulaires (celles stockées dans des DataFrames Pandas). Avec un ajustement minutieux des paramètres, vous pouvez entraîner des modèles très précis.\n",
    "\n",
    "---\n",
    "\n",
    "### À vous de jouer !\n",
    "\n",
    "Entraînez votre propre modèle avec XGBoost dans le prochain exercice ! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecf59c-5ee9-4956-b80b-ee2a1bc3cecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
